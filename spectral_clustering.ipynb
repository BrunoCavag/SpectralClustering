{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Clustering\n",
    "\n",
    "Now we will explore the ins and outs of spectral clustering for graph and other data. Spectral clustering is a technique with roots in graph theory, where the approach is used to identifyu commnunities of nodes in graph based on the edges connecting them. This method isn't exclusive to cluster, it can work with other kinds of data. \n",
    "\n",
    "Spectral clustering uses information from the eigenvalues (spectrum) of special matrices built from the graph or the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvectors and eigenvalues\n",
    "\n",
    "We can think of a matrix A as a function which map vector to new vectors. Most vectors will end up somewhere completly different when A is applied to them, but eigenvectors only changes in magnitude. If you drew a line through the origin and the eigenvector, then after the mapping, the eigenvector would still land on the line. The amount which the vector is scaled along the line depends on $\\lambda$ (eigenvalue).\n",
    "\n",
    "Eigenvectors are an important part of linear algebra, because they help describe the dynamics of systems represented by matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvector: [-0.4472136   0.89442719] Eigenvalue: -1.0\n",
      "Eigenvector: [-0.4472136   0.89442719] Eigenvalue: -2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[0,1],[-2,-3]])\n",
    "\n",
    "vals, vecs = np.linalg.eig(A)\n",
    "\n",
    "for i, value in enumerate(vals):\n",
    "    print(\"Eigenvector:\", vecs[:,1],\"Eigenvalue:\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n",
    "\n",
    "Graphs are natural way to reprsent many types of data. A graph is a set of nodes with a corresponding set of edges which connect the nodes. The edges may be directed or undirected and can even have weights associated with them. \n",
    "\n",
    "## Degree Matrix\n",
    "\n",
    "The degree of a node is how many edges connect to it. In a directed graph we could talk about in-degree and out-degree. The degree matrix is a diagonal matrix where the value at entry (i,i) is the degree of node i. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0 0 0 0]\n",
      " [0 0 2 0 0 0 0 0 0 0]\n",
      " [0 0 0 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 2 0 0 0 0 0]\n",
      " [0 0 0 0 0 4 0 0 0 0]\n",
      " [0 0 0 0 0 0 2 0 0 0]\n",
      " [0 0 0 0 0 0 0 2 0 0]\n",
      " [0 0 0 0 0 0 0 0 2 0]\n",
      " [0 0 0 0 0 0 0 0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "  [0, 1, 1, 0, 0, 0, 0, 0, 1, 1],\n",
    "  [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 0, 1, 1, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
    "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "  [1, 0, 0, 0, 0, 0, 0, 0, 1, 0]])\n",
    "\n",
    "D = np.diag(A.sum(axis=1))\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Laplacian\n",
    "\n",
    "The laplacian is just another matrix representation of a graph. It has several beautiful properties, which we will take advange of for spectral clustering. In fact, the number of 0 eigenvalues corresponds to the number of connected components in our graph! To calculate the normal Laplacian (there are several variants), we just subtract the adjacency matrix from our degree matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4 -1 -1  0  0  0  0  0 -1 -1]\n",
      " [-1  2 -1  0  0  0  0  0  0  0]\n",
      " [-1 -1  2  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  2 -1 -1  0  0  0  0]\n",
      " [ 0  0  0 -1  2 -1  0  0  0  0]\n",
      " [ 0  0  0 -1 -1  4 -1 -1  0  0]\n",
      " [ 0  0  0  0  0 -1  2 -1  0  0]\n",
      " [ 0  0  0  0  0 -1 -1  2  0  0]\n",
      " [-1  0  0  0  0  0  0  0  2 -1]\n",
      " [-1  0  0  0  0  0  0  0 -1  2]]\n"
     ]
    }
   ],
   "source": [
    "L = D - A\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first nonzero eigenvalue is called the spectral gap. The spectral gap gives us some notion of the density of the graph. If this graph was densely connected (all pairs of the 10 nodes had an edge), then the spectral gap would be 10.\n",
    "\n",
    "The second eigenvalue is called the Fiedler value, and the corresponding vector is the Fiedler vector. The Fiedler value approximates the minimum graph cut needed to separate the graph into two connected components. Recall, that if our graph was already two connected components, then the Fiedler value would be 0. Each value in the Fiedler vector gives us information about which side of the cut that node belongs. Let’s color the nodes based on whether their entry in the Fielder vector is positive or not:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple trick has segmented our graph into two clusters! Why does this work? Remember that zero eigenvalues represent connected components. Eigenvalues near zero are telling us that there is almost a separation of two components. Here we have a single edge, that if it didn’t exist, we’d have two separate components. So the second eigenvalue is small.\n",
    "\n",
    "To summarize what we know so far: the first eigenvalue is 0 because we have one connected component. The second eigenvalue is near 0 because we’re one edge away from having two connected components. We also saw that the vector associated with that value tells us how to separate the nodes into those approximately connected components.\n",
    "\n",
    "You may have noticed that the next two eigenvalues are also pretty small. That tells us that we are “close” to having four separate connected components. In general, we often look for the first large gap between eigenvalues in order to find the number of clusters expressed in our data. See the gap between eigenvalues four and five?\n",
    "\n",
    "Having four eigenvalues before the gap indicates that there is likely four clusters. The vectors associated with the first three positive eigenvalues should give us information about which three cuts need to be made in the graph to assign each node to one of the four approximated components. Let’s build a matrix from these three vectors and perform K-Means clustering to determine the assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency Matrix:\n",
      "[[0 1 1 0 0 0 0 0 1 1]\n",
      " [1 0 1 0 0 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 1 0 0 0 0]\n",
      " [0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 1 1 0 1 1 0 0]\n",
      " [0 0 0 0 0 1 0 1 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 1 0]]\n",
      "Clusters: [2 1 1 3 3 0 0 0 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# our adjacency matrix\n",
    "print(\"Adjacency Matrix:\")\n",
    "print(A)\n",
    "\n",
    "# diagonal matrix\n",
    "D = np.diag(A.sum(axis=1))\n",
    "\n",
    "# graph laplacian\n",
    "L = D-A\n",
    "\n",
    "# eigenvalues and eigenvectors\n",
    "vals, vecs = np.linalg.eig(L)\n",
    "\n",
    "# sort these based on the eigenvalues\n",
    "vecs = vecs[:,np.argsort(vals)]\n",
    "vals = vals[np.argsort(vals)]\n",
    "\n",
    "# kmeans on first three vectors with nonzero eigenvalues\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(vecs[:,1:4])\n",
    "colors = kmeans.labels_\n",
    "\n",
    "print(\"Clusters:\", colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering Arbitrary Data \n",
    "\n",
    "Obviously, K-Means wasn’t going to work. It operates on euclidean distance, and it assumes that the clusters are roughly spherical. This data (and often real world data) breaks these assumptions. Let’s try to tackle this with spectral clustering.\n",
    "\n",
    "### Nearest Neighbors Graph\n",
    "\n",
    "There are a couple of ways to treat our data as a graph. The easiest way is to construct a k-nearest neighbors graph. A k-nearest neighbors graph treats every data point as a node in a graph. An edge is then drawn from each node to its k nearest neighbors in the original space. Generally, the algorithm isn’t too sensitive of the choice of k. Smaller numbers like 5 or 10 usually work pretty well.\n",
    "\n",
    "Look at the picture of the data again, and imagine that each point is connected to its 5 closest neighbors. Any point in the outer ring should be able to follow a path along the ring, but there won’t be any paths into the inner circle. It’s pretty easy to see that this graph will have two connected components: the outer ring and the inner circle.\n",
    "\n",
    "Since we’re only separating this data into two components, we should be able to use our Fiedler vector trick from before. Here’s the code I used to perform spectral clustering on this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import numpy as np\n",
    "\n",
    "# create the data\n",
    "X, labels = make_circles(n_samples=500, noise=0.1, factor=.2)\n",
    "\n",
    "# use the nearest neighbor graph as our adjacency matrix\n",
    "A = kneighbors_graph(X, n_neighbors=5).toarray()\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the graph laplacian\n",
    "D = np.diag(A.sum(axis=1))\n",
    "L = D-A\n",
    "\n",
    "# find the eigenvalues and eigenvectors\n",
    "vals, vecs = np.linalg.eig(L)\n",
    "\n",
    "# sort\n",
    "vecs = vecs[:,np.argsort(vals)]\n",
    "vals = vals[np.argsort(vals)]\n",
    "\n",
    "# use Fiedler value to find best cut to separate data\n",
    "clusters = vecs[:,1] > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Approaches\n",
    "\n",
    "The nearest neighbor graph is a nice approach, but it relies on the fact that “close” points should belong in the same cluster. Depending on your data, that might not be true. A more general approach is to construct an affinity matrix. An affinity matrix is just like an adjacency matrix, except the value for a pair of points expresses how similar those points are to each other. If pairs of points are very dissimilar then the affinity should be 0. If the points are identical, then the affinity might be 1. In this way, the affinity acts like the weights for the edges on our graph.\n",
    "\n",
    "How to decide on what it means for two data points to be similar is one of the most important questions in machine learning. Often domain knowledge is the best way to construct a similarity measure. If you have access to domain experts, ask them this question.\n",
    "\n",
    "There are also entire fields dedicated to learning how to construct similarity metrics directly from data. For example, if you have some labeled data, you can train a classifier to predict whether two inputs are similar or not based on if they have the same label. This classifier can then be used to assign affinity to pairs of unlabeled points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We’ve covered the theory and application of spectral clustering for both graphs and arbitrary data. Spectral clustering is a flexible approach for finding clusters when your data doesn’t meet the requirements of other common algorithms.\n",
    "\n",
    "First, we formed a graph between our data points. The edges of the graph capture the similarities between the points. The eigenvalues of the Graph Laplacian can then be used to find the best number of clusters, and the eigenvectors can be used to find the actual cluster labels.\n",
    "\n",
    "I hope you enjoyed this post and find spectral clustering useful in your work or exploration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('spectral_clustering': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b36883457548d729e7459790872fc52e80ca3bf04ac39dadcbba9db79bac5912"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
